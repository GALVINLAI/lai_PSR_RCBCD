{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of JAX's Automatic Differentiation for Computation of Gradient and Hessian Matrix\n",
    "\n",
    "This document demonstrates how to use the JAX library to compute the gradient and Hessian matrix of a multivariable function and verify the correctness of the results. \n",
    "\n",
    "We use a simple quadratic objective function $f(x) = \\|Ax - b\\|^2$ as an example.\n",
    "\n",
    "## Main Contents\n",
    "\n",
    "1. **Objective Function Definition**: Define a multivariable quadratic function $f(x) = \\|Ax - b\\|^2$, where $A$ is a matrix, and $x$ and $b$ are vectors.\n",
    "2. **Random Data Generation**: Randomly generate the matrix $A$, vectors $b$, and $x$, allowing the user to set their dimensions.\n",
    "3. **Gradient Computation**: Use JAX's `grad` function to compute the gradient of the objective function.\n",
    "4. **Hessian Matrix Computation**: Use JAX's `hessian` function to compute the Hessian matrix of the objective function.\n",
    "5. **Result Verification**: Calculate the analytical form of the gradient and Hessian matrix and compare them with the results computed by JAX to verify their consistency.\n",
    "\n",
    "The following code implementation and detailed comments will help readers understand each step's operations and principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient: [8.0490675 5.0572586 8.928235 ]\n",
      "JAX Gradient: [8.0490675 5.0572586 8.928235 ]\n",
      "Analytical Hessian: \n",
      "[[4.31184    0.53901273 1.7090865 ]\n",
      " [0.53901273 7.4374714  3.4608386 ]\n",
      " [1.7090865  3.4608386  5.548139  ]]\n",
      "JAX Hessian: \n",
      "[[4.31184    0.53901273 1.7090865 ]\n",
      " [0.53901273 7.4374714  3.4608386 ]\n",
      " [1.7090865  3.4608386  5.548139  ]]\n",
      "==================================================\n",
      "The gradients and Hessians match!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, hessian\n",
    "import numpy as np\n",
    "\n",
    "# Define the dimensions of the matrix and vectors\n",
    "m, n = 4, 3  # For example, A is a 4x3 matrix, b is a vector of length 4, and x is a vector of length 3\n",
    "\n",
    "# Randomly generate the matrix A and vector b\n",
    "A = jnp.array(np.random.randn(m, n))\n",
    "b = jnp.array(np.random.randn(m))\n",
    "\n",
    "# Randomly generate the test point x\n",
    "x = jnp.array(np.random.randn(n))\n",
    "\n",
    "# Define the objective function f(x) = ||Ax - b||^2\n",
    "def objective_function(x):\n",
    "    return jnp.sum((jnp.dot(A, x) - b) ** 2)\n",
    "\n",
    "# Compute the gradient of the objective function using JAX's grad function\n",
    "grad_function = grad(objective_function)\n",
    "\n",
    "# Compute the Hessian matrix of the objective function using JAX's hessian function\n",
    "hess_function = hessian(objective_function)\n",
    "\n",
    "# Compute the analytical gradient\n",
    "# The analytical gradient formula is 2 * A^T * (A * x - b)\n",
    "analytical_grad = 2 * jnp.dot(A.T, jnp.dot(A, x) - b)\n",
    "\n",
    "# Compute the gradient using JAX\n",
    "jax_grad = grad_function(x)\n",
    "\n",
    "# Compute the analytical Hessian matrix\n",
    "# The analytical Hessian matrix formula is 2 * A^T * A\n",
    "analytical_hess = 2 * jnp.dot(A.T, A)\n",
    "\n",
    "# Compute the Hessian matrix using JAX\n",
    "jax_hess = hess_function(x)\n",
    "\n",
    "# Print the analytical gradient and the gradient computed by JAX\n",
    "print(f\"Analytical Gradient: {analytical_grad}\")\n",
    "print(f\"JAX Gradient: {jax_grad}\")\n",
    "\n",
    "# Print the analytical Hessian and the Hessian computed by JAX\n",
    "print(f\"Analytical Hessian: \\n{analytical_hess}\")\n",
    "print(f\"JAX Hessian: \\n{jax_hess}\")\n",
    "\n",
    "# Verify if the results are consistent\n",
    "print('='*50)\n",
    "assert jnp.allclose(analytical_grad, jax_grad, rtol=1e-05, atol=1e-06), \"The gradients do not match!\"\n",
    "assert jnp.allclose(analytical_hess, jax_hess, rtol=1e-05, atol=1e-06), \"The Hessians do not match!\"\n",
    "print(\"The gradients and Hessians match!\")\n",
    "print('='*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Do Not Match?\n",
    "\n",
    "This is likely due to numerical errors in floating-point calculations. \n",
    "\n",
    "Increasing the tolerance parameters in `allclose` can resolve this issue. These errors tend to increase as the scale of variables grows. However, theoretically, the computed results should be the same.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The key point in using JAX is to use `jax.numpy` instead of `numpy` to describe the objective function. Other than that, there are no specific requirements.\n",
    "\n",
    "## Another Example\n",
    "\n",
    "Consider a more complex objective function, $\\operatorname{tr}\\left(A \\cdot X^{\\top} \\cdot B \\cdot X \\cdot C\\right)$, where A is a symmetric matrix. According to [Matrix Calculus](https://www.matrixcalculus.org/), the analytical expression for the gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X}\\left(\\operatorname{tr}\\left(A \\cdot X^{\\top} \\cdot B \\cdot X \\cdot C\\right)\\right)=B \\cdot X \\cdot C \\cdot A+B^{\\top} \\cdot X \\cdot A \\cdot C^{\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient: \n",
      "[[  3.7105083 -19.493462    8.29455   -15.67075    -1.8323956]\n",
      " [ -5.695397   13.323085   -3.8245463   3.06843     8.027785 ]\n",
      " [  3.8335967  -8.7685      7.812145  -22.5878     -9.239664 ]\n",
      " [  6.060854  -17.89624     1.8304543  15.615094   -5.914414 ]\n",
      " [ -6.886673   26.397444   -7.059568  -13.699003    5.1156425]]\n",
      "JAX Gradient: \n",
      "[[  3.7105088 -19.493464    8.294551  -15.670748   -1.8323936]\n",
      " [ -5.695398   13.323086   -3.8245468   3.0684295   8.027785 ]\n",
      " [  3.8335967  -8.768501    7.8121448 -22.587801   -9.239666 ]\n",
      " [  6.060856  -17.89624     1.8304546  15.615095   -5.9144144]\n",
      " [ -6.886676   26.397442   -7.0595675 -13.699005    5.1156445]]\n",
      "==================================================\n",
      "The gradients match!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import numpy as np\n",
    "\n",
    "# Define the dimensions of the matrices\n",
    "n = 5\n",
    "\n",
    "# Randomly generate matrices A, B, C, and ensure A is symmetric\n",
    "# np.random.seed(0)  # To ensure reproducibility, set a random seed\n",
    "A = np.random.randn(n, n)\n",
    "A = (A + A.T) / 2  # Ensure A is symmetric\n",
    "A = jnp.array(A)\n",
    "B = jnp.array(np.random.randn(n, n))\n",
    "C = jnp.array(np.random.randn(n, n))\n",
    "\n",
    "# Randomly generate the test point X\n",
    "X = jnp.array(np.random.randn(n, n))\n",
    "\n",
    "# Define the objective function f(X) = tr(A * X' * B * X * C)\n",
    "def objective_function(X):\n",
    "    return jnp.trace(jnp.dot(A, jnp.dot(X.T, jnp.dot(B, jnp.dot(X, C)))))\n",
    "\n",
    "# Compute the gradient of the objective function using JAX's grad function\n",
    "grad_function = grad(objective_function)\n",
    "\n",
    "# Compute the analytical gradient\n",
    "# The analytical gradient formula is B * X * C * A + B.T * X * A * C.T\n",
    "analytical_grad = jnp.dot(B, jnp.dot(X, jnp.dot(C, A))) + jnp.dot(B.T, jnp.dot(X, jnp.dot(A, C.T)))\n",
    "\n",
    "# Compute the gradient using JAX\n",
    "jax_grad = grad_function(X)\n",
    "\n",
    "# Print the analytical gradient and the gradient computed by JAX\n",
    "print(f\"Analytical Gradient: \\n{analytical_grad}\")\n",
    "print(f\"JAX Gradient: \\n{jax_grad}\")\n",
    "\n",
    "# Verify if the results are consistent\n",
    "print('='*50)\n",
    "assert jnp.allclose(analytical_grad, jax_grad, rtol=1e-05, atol=1e-05), \"The gradients do not match!\"\n",
    "print(\"The gradients match!\")\n",
    "print('='*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
