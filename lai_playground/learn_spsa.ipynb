{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous perturbation stochastic approximation\n",
    "\n",
    "SPSA is a descent method capable of finding global minima, sharing this property with other methods such as simulated annealing. Its main feature is the gradient approximation that requires only two measurements of the objective function, regardless of the dimension of the optimization problem. Recall that we want to find the optimal control $u^*$ with loss function $J(u)$ :\n",
    "$$\n",
    "u^*=\\arg \\min _{u \\in U} J(u)\n",
    "$$\n",
    "\n",
    "Both Finite Differences Stochastic Approximation (FDSA) and SPSA use the same iterative process:\n",
    "$$\n",
    "u_{n+1}=u_n-a_n \\hat{g}_n\\left(u_n\\right) \\text {, }\n",
    "$$\n",
    "where $u_n=\\left(\\left(u_n\\right)_1,\\left(u_n\\right)_2, \\ldots,\\left(u_n\\right)_p\\right)^T$ represents the $n^{t h}$ iterate, $\\hat{g}_n\\left(u_n\\right)$ is the estimate of the gradient of the objective function $g(u)=\\frac{\\partial}{\\partial u} J(u)$ evaluated at $u_n$, and $\\left\\{a_n\\right\\}$ is a positive number sequence converging to 0 . If $u_n$ is a $p$ dimensional vector, the $i^{\\text {th }}$ component of the symmetric finite difference gradient estimator is:\n",
    "$$\n",
    "\\text { FD: }\\left(\\hat{g_n}\\left(u_n\\right)\\right)_i=\\frac{J\\left(u_n+c_n e_i\\right)-J\\left(u_n-c_n e_i\\right)}{2 c_n},\n",
    "$$\n",
    "$1 \\leq i \\leq p$, where $e_i$ is the unit vector with a 1 in the $i^{\\text {th }}$ place, and $c_n$ is a small positive number that decreases with $n$. With this method, $2 p$ evaluations of $J$ for each $g_n$ are needed. When $p$ is large, this estimator loses efficiency.\n",
    "\n",
    "Let now $\\Delta_n$ be a random perturbation vector. The $i^{\\text {th }}$ component of the stochastic perturbation gradient estimator is:\n",
    "$$\n",
    "\\text { SP: }\\left(\\hat{g_n}\\left(u_n\\right)\\right)_i=\\frac{J\\left(u_n+c_n \\Delta_n\\right)-J\\left(u_n-c_n \\Delta_n\\right)}{2 c_n\\left(\\Delta_n\\right)_i} \\text {. }\n",
    "$$\n",
    "\n",
    "Remark that FD perturbs only one direction at a time, while the SP estimator disturbs all directions at the same time (the numerator is identical in all $p$ components). The number of loss function measurements needed in the SPSA method for each $g_n$ is always 2, independent of the dimension $p$. Thus, SPSA uses $p$ times fewer function evaluations than FDSA, which makes it a lot more efficient.\n",
    "\n",
    "Simple experiments with $p=2$ showed that SPSA converges in the same number of iterations as FDSA. The latter follows approximately the steepest descent direction, behaving like the gradient method. On the other hand, SPSA, with the random search direction, does not follow exactly the gradient path. In average though, it tracks it nearly because the gradient approximation is an almost unbiased estimator of the gradient, as shown in the following lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence lemma\n",
    "\n",
    "Denote by\n",
    "$$\n",
    "b_n=E\\left[\\hat{g}_n \\mid u_n\\right]-\\nabla J\\left(u_n\\right)\n",
    "$$\n",
    "the bias in the estimator $\\hat{g}_n$. Assume that $\\left\\{\\left(\\Delta_n\\right)_i\\right\\}$ are all mutually independent with zero-mean, bounded second moments, and $E\\left(\\left|\\left(\\Delta_n\\right)_i\\right|^{-1}\\right)$ uniformly bounded. Then $b_n \\rightarrow 0$ w.p. 1.\n",
    "\n",
    "Proof. https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: theta = [0.50337427 0.22430709 0.88794443 0.5638395  0.40357643], loss = 1.5729335595668348\n",
      "Iteration 2: theta = [ 0.81863291 -0.09095155  0.57268579  0.24858086  0.08831779], loss = 1.07599351183515\n",
      "Iteration 3: theta = [ 0.81863291 -0.09095155  0.57268579  0.24858086  0.08831779], loss = 1.07599351183515\n",
      "Iteration 4: theta = [ 0.62593921 -0.28364524  0.37999209  0.44127456  0.28101149], loss = 0.8903392043442312\n",
      "Iteration 5: theta = [ 0.48902162 -0.42056283  0.51690968  0.30435698  0.14409391], loss = 0.7966070787530618\n",
      "Iteration 6: theta = [ 0.23577541 -0.16731662  0.26366347  0.55760319 -0.1091523 ], loss = 0.47593886048585127\n",
      "Iteration 7: theta = [ 0.03599986 -0.36709217  0.06388792  0.35782764  0.09062325], loss = 0.27638750745271756\n",
      "Iteration 8: theta = [ 0.17913091 -0.22396111  0.20701897  0.21469658 -0.05250781], loss = 0.17395501203396055\n",
      "Iteration 9: theta = [ 0.10747855 -0.15230875  0.27867134  0.14304422 -0.12416018], loss = 0.14828470429926194\n",
      "Iteration 10: theta = [ 0.10747855 -0.15230875  0.27867134  0.14304422 -0.12416018], loss = 0.14828470429926194\n",
      "Iteration 11: theta = [-0.00398999 -0.04084021  0.1672028   0.03157568 -0.23562871], loss = 0.08615853277947416\n",
      "Iteration 12: theta = [ 0.0123461  -0.02450413  0.18353889  0.04791177 -0.21929263], loss = 0.084824194497079\n",
      "Iteration 13: theta = [ 0.10492636  0.06807613  0.09095863 -0.04466849 -0.12671236], loss = 0.041968670314703124\n",
      "Iteration 14: theta = [ 0.05424141  0.01739119  0.14164357  0.00601645 -0.07602742], loss = 0.029123851755138472\n",
      "Iteration 15: theta = [ 0.09160886 -0.01997625  0.10427613 -0.03135099 -0.03865998], loss = 0.022142222748894765\n",
      "Iteration 16: theta = [ 0.0704293  -0.04115581  0.08309658 -0.05253054 -0.05983953], loss = 0.019899355458885562\n",
      "Iteration 17: theta = [ 0.0704293  -0.04115581  0.08309658 -0.05253054 -0.05983953], loss = 0.01989935545888557\n",
      "Iteration 18: theta = [ 0.04649349 -0.06509162  0.05916076 -0.07646636 -0.03590372], loss = 0.017034740152256256\n",
      "Iteration 19: theta = [ 0.05393274 -0.05765237  0.05172151 -0.08390561 -0.02846447], loss = 0.016758027961334403\n",
      "Iteration 20: theta = [ 0.04254696 -0.06903815  0.0631073  -0.07251982 -0.01707868], loss = 0.01610984725285967\n",
      "Iteration 21: theta = [ 0.07838636 -0.03319876  0.0272679  -0.03668042  0.01876072], loss = 0.009687534367124296\n",
      "Iteration 22: theta = [ 0.04703181 -0.00184421 -0.00408664 -0.00532588  0.05011526], loss = 0.0047719977059638\n",
      "Iteration 23: theta = [ 0.0477695  -0.00110653 -0.00334896 -0.00606356  0.04937758], loss = 0.004769276809650243\n",
      "Iteration 24: theta = [ 0.04841273 -0.00174976 -0.00399219 -0.00542033  0.04873435], loss = 0.0047672080773026645\n",
      "Iteration 25: theta = [ 0.02834773  0.01831524 -0.02405718  0.01464466  0.02866935], loss = 0.0027541878436441163\n",
      "Iteration 26: theta = [ 0.01516377  0.00513128 -0.03724114  0.0014607   0.01548539], loss = 0.0018851037607511014\n",
      "Iteration 27: theta = [ 0.00909826  0.01119679 -0.03117563  0.00752621  0.0215509 ], loss = 0.0017011517301469038\n",
      "Iteration 28: theta = [ 0.00846944  0.01056797 -0.03180445  0.00815503  0.02092208], loss = 0.001699174655711105\n",
      "Iteration 29: theta = [ 0.00943462  0.00960279 -0.03083928  0.00912021  0.02188725], loss = 0.0016945168651977297\n",
      "Iteration 30: theta = [-0.0029011   0.02193851 -0.01850357 -0.00321551  0.00955154], loss = 0.0009336678241194374\n",
      "Iteration 31: theta = [-0.00914209  0.01569752 -0.01226258  0.00302548  0.01579253], loss = 0.0007389180924536263\n",
      "Iteration 32: theta = [-0.01039028  0.01694571 -0.01101438  0.00177729  0.01454433], loss = 0.0007311281031869938\n",
      "Iteration 33: theta = [-0.00623417  0.02110183 -0.00685827 -0.00237883  0.01038822], loss = 0.0006447617168717745\n",
      "Iteration 34: theta = [-0.00194873  0.01681638 -0.00257282 -0.00666427  0.01467366], loss = 0.0005529365550951246\n",
      "Iteration 35: theta = [ 0.00289161  0.01197605 -0.00741316 -0.01150461  0.00983333], loss = 0.0004357923593961394\n",
      "Iteration 36: theta = [ 0.00585687  0.00901078 -0.0044479  -0.01446987  0.00686806], loss = 0.00039182841654338837\n",
      "Iteration 37: theta = [ 0.00529328  0.00844719 -0.00501149 -0.01503346  0.00630447], loss = 0.00039024024322639554\n",
      "Iteration 38: theta = [ 0.00666756  0.00707291 -0.00638577 -0.01365918  0.00767876], loss = 0.0003807969733291566\n",
      "Iteration 39: theta = [ 0.0038384   0.00424375 -0.00355661 -0.01648834  0.00484959], loss = 0.00034077611763392744\n",
      "Iteration 40: theta = [ 0.00060554  0.00101089 -0.00678946 -0.01325548  0.00808245], loss = 0.0002885192367549688\n",
      "Iteration 41: theta = [ 0.00307911 -0.00146268 -0.00431589 -0.01078192  0.01055602], loss = 0.0002579265215827216\n",
      "Iteration 42: theta = [ 0.00298875 -0.00137232 -0.00440625 -0.01069156  0.01064638], loss = 0.00025788569813332113\n",
      "Iteration 43: theta = [ 0.0060518   0.00169073 -0.0074693  -0.00762851  0.00758333], loss = 0.0002109743199420308\n",
      "Iteration 44: theta = [ 0.00367669  0.00406584 -0.00509419 -0.00525339  0.00995844], loss = 0.00018276854739288014\n",
      "Iteration 45: theta = [ 0.00367669  0.00406584 -0.00509419 -0.00525339  0.00995844], loss = 0.00018276854739288014\n",
      "Iteration 46: theta = [ 0.00577805  0.00196448 -0.00719555 -0.00315204  0.00785708], loss = 0.00016069002175926521\n",
      "Iteration 47: theta = [ 0.00346683 -0.00034674 -0.00950677 -0.00084082  0.00554586], loss = 0.00013398134737279554\n",
      "Iteration 48: theta = [ 0.00332813 -0.00020804 -0.00964546 -0.00070212  0.00540717], loss = 0.00013388516547018035\n",
      "Iteration 49: theta = [ 0.005491   -0.00237091 -0.0074826  -0.00286499  0.0032443 ], loss = 0.00011049521128880682\n",
      "Iteration 50: theta = [ 0.00514164 -0.00272027 -0.00783195 -0.00251563  0.00289494], loss = 0.00010988496285937758\n",
      "Iteration 51: theta = [ 0.00092076  0.00150062 -0.00361107  0.00170526 -0.00132594], loss = 2.080548679761249e-05\n",
      "Iteration 52: theta = [ 0.0011527   0.00126868 -0.00337912  0.0019372  -0.00155789], loss = 2.053649060394574e-05\n",
      "Iteration 53: theta = [ 0.0011527   0.00126868 -0.00337912  0.0019372  -0.00155789], loss = 2.053649060394574e-05\n",
      "Iteration 54: theta = [-0.00019895  0.00262032 -0.00202747  0.00058555 -0.00020624], loss = 1.1401729294181862e-05\n",
      "Iteration 55: theta = [ 3.52763170e-05  2.38610230e-03 -2.26169361e-03  3.51332466e-04\n",
      " -4.40464839e-04], loss = 1.1127430401838885e-05\n",
      "Iteration 56: theta = [-0.00088351  0.00146731 -0.00134291 -0.00056746 -0.00135925], loss = 6.906573710968614e-06\n",
      "Iteration 57: theta = [ 0.00024058  0.00034323 -0.00021882  0.00055663 -0.00023516], loss = 5.887046735925775e-07\n",
      "Iteration 58: theta = [ 0.00010329  0.00020594 -0.00035611  0.00041934 -0.00037246], loss = 4.944612159971161e-07\n",
      "Iteration 59: theta = [ 0.00018566  0.00012356 -0.00027373  0.00050172 -0.00029008], loss = 4.605335712627471e-07\n",
      "Iteration 60: theta = [ 6.96277044e-05  2.39593898e-04 -3.89766282e-04  3.85683853e-04\n",
      " -1.74048622e-04], loss = 3.932159650118452e-07\n",
      "Iteration 61: theta = [ 4.34095940e-05  2.13375788e-04 -4.15984392e-04  3.59465743e-04\n",
      " -2.00266732e-04], loss = 3.8977901846042246e-07\n",
      "Iteration 62: theta = [-3.76338478e-05  2.94419229e-04 -3.34940950e-04  2.78422301e-04\n",
      " -2.81310174e-04], loss = 3.5693882112239497e-07\n",
      "Iteration 63: theta = [-2.67925609e-04  6.41274679e-05 -1.04649189e-04  4.81305396e-05\n",
      " -5.10184128e-05], loss = 9.176734424151573e-08\n",
      "Iteration 64: theta = [-2.28266028e-04  1.03787049e-04 -1.44308770e-04  8.47095868e-06\n",
      " -1.13588318e-05], loss = 8.390293242965008e-08\n",
      "Iteration 65: theta = [-1.86751209e-04  6.22722293e-05 -1.85823589e-04 -3.30438609e-05\n",
      "  3.01559878e-05], loss = 7.528553121298652e-08\n",
      "Iteration 66: theta = [-1.12421773e-04 -1.20572064e-05 -1.11494154e-04 -1.07373297e-04\n",
      "  1.04485424e-04], loss = 4.766120611186657e-08\n",
      "Iteration 67: theta = [-7.22759463e-05 -5.22030330e-05 -1.51639980e-04 -6.72274700e-05\n",
      "  6.43395969e-05], loss = 3.9602769141314136e-08\n",
      "Iteration 68: theta = [-1.76297290e-05  2.44318427e-06 -9.69937630e-05 -1.21873687e-04\n",
      "  9.69337962e-06], loss = 2.4671723817594157e-08\n",
      "Iteration 69: theta = [-2.15070809e-05 -1.43416757e-06 -1.00871115e-04 -1.17996335e-04\n",
      "  5.81602777e-06], loss = 2.45965545308926e-08\n",
      "Iteration 70: theta = [ 2.80178644e-05  4.80907777e-05 -5.13461695e-05 -6.84713902e-05\n",
      " -4.37089175e-05], loss = 1.2332953496052382e-08\n",
      "Iteration 71: theta = [ 2.67157077e-05  4.93929344e-05 -5.00440128e-05 -6.97735469e-05\n",
      " -4.24067608e-05], loss = 1.2324475435583139e-08\n",
      "Iteration 72: theta = [ 4.67333128e-05  2.93753293e-05 -3.00264077e-05 -4.97559418e-05\n",
      " -6.24243659e-05], loss = 1.0320952862763161e-08\n",
      "Iteration 73: theta = [ 2.29726180e-05  5.61463452e-06 -6.26571288e-06 -7.35166366e-05\n",
      " -3.86636711e-05], loss = 7.498099776588702e-09\n",
      "Iteration 74: theta = [ 1.12772857e-05  1.73099669e-05  5.42961949e-06 -6.18213042e-05\n",
      " -5.03590035e-05], loss = 6.81419578116189e-09\n",
      "Iteration 75: theta = [ 2.69099728e-05  3.29426540e-05  2.10623066e-05 -4.61886171e-05\n",
      " -3.47263164e-05], loss = 5.5922912459978346e-09\n",
      "Iteration 76: theta = [ 3.00365102e-05  2.98161166e-05  1.79357692e-05 -4.93151545e-05\n",
      " -3.15997789e-05], loss = 5.543415064591285e-09\n",
      "Iteration 77: theta = [-1.70415566e-06 -1.92454930e-06 -1.38048967e-05 -1.75744886e-05\n",
      "  1.40886949e-07], loss = 5.060657083358704e-10\n",
      "Iteration 78: theta = [-7.99593406e-06  4.36722910e-06 -7.51311830e-06 -1.12827102e-05\n",
      "  6.43266535e-06], loss = 3.081333312416019e-10\n",
      "Iteration 79: theta = [-7.56375288e-06  3.93504792e-06 -7.08093712e-06 -1.17148914e-05\n",
      "  6.86484653e-06], loss = 3.0719942837785263e-10\n",
      "Iteration 80: theta = [-1.03961277e-05  6.76742277e-06 -4.24856227e-06 -8.88251654e-06\n",
      "  4.03247168e-06], loss = 2.6708769199723137e-10\n",
      "Iteration 81: theta = [-8.78313906e-06  5.15443409e-06 -5.86155094e-06 -1.04955052e-05\n",
      "  2.41948301e-06], loss = 2.5407902971177407e-10\n",
      "Iteration 82: theta = [-1.08449127e-05  3.09266046e-06 -3.79977731e-06 -8.43373158e-06\n",
      "  4.48125665e-06], loss = 2.328244770470342e-10\n",
      "Iteration 83: theta = [-9.88044025e-06  2.12818801e-06 -2.83530486e-06 -9.39820402e-06\n",
      "  5.44572909e-06], loss = 2.281734415473935e-10\n",
      "Iteration 84: theta = [-5.07698894e-06 -2.67526329e-06 -7.63875616e-06 -4.59475272e-06\n",
      "  6.42277790e-07], loss = 1.1280771945523405e-10\n",
      "Iteration 85: theta = [-6.10169583e-06 -3.69997018e-06 -6.61404927e-06 -3.57004583e-06\n",
      " -3.82429097e-07], loss = 1.0755759843195822e-10\n",
      "Iteration 86: theta = [-3.45607612e-06 -1.05435047e-06 -3.96842956e-06 -6.21566554e-06\n",
      "  2.26319061e-06], loss = 7.256108018286e-11\n",
      "Iteration 87: theta = [-2.97254006e-06 -1.53788653e-06 -3.48489351e-06 -6.69920160e-06\n",
      "  1.77965456e-06], loss = 7.139204459058539e-11\n",
      "Iteration 88: theta = [-4.46320468e-06 -3.02855114e-06 -1.99422889e-06 -5.20853699e-06\n",
      "  2.88989941e-07], loss = 6.028163962481897e-11\n",
      "Iteration 89: theta = [-2.37978988e-06 -9.45136347e-07 -4.07764369e-06 -3.12512219e-06\n",
      "  2.37240474e-06], loss = 3.8578553595536316e-11\n",
      "Iteration 90: theta = [-3.05893141e-06 -2.65994826e-07 -3.39850217e-06 -2.44598067e-06\n",
      "  3.05154626e-06], loss = 3.6272387565359e-11\n",
      "Iteration 91: theta = [-3.19475971e-06 -4.01823130e-07 -3.26267386e-06 -2.58180897e-06\n",
      "  2.91571795e-06], loss = 3.618014092414474e-11\n",
      "Iteration 92: theta = [-4.22748330e-06 -1.43454672e-06 -2.22995027e-06 -1.54908538e-06\n",
      "  1.88299436e-06], loss = 3.084755085895672e-11\n",
      "Iteration 93: theta = [-2.53648998e-06 -3.12554004e-06 -5.38956952e-07  1.41907935e-07\n",
      "  1.92001043e-07], loss = 1.655025881799896e-11\n",
      "Iteration 94: theta = [-1.22951079e-06 -1.81856085e-06  7.68022238e-07 -1.16507125e-06\n",
      " -1.11497815e-06], loss = 8.00928580240941e-12\n",
      "Iteration 95: theta = [-1.02821340e-08 -5.99332194e-07 -4.51206418e-07  5.41574011e-08\n",
      "  1.04250509e-07], loss = 5.76693225465181e-13\n",
      "Iteration 96: theta = [-4.78694839e-08 -5.61744844e-07 -4.88793768e-07  9.17447509e-08\n",
      "  6.66631588e-08], loss = 5.696291811330709e-13\n",
      "Iteration 97: theta = [-4.78694839e-08 -5.61744844e-07 -4.88793768e-07  9.17447509e-08\n",
      "  6.66631588e-08], loss = 5.696291811330708e-13\n",
      "Iteration 98: theta = [-4.78694839e-08 -5.61744844e-07 -4.88793768e-07  9.17447509e-08\n",
      "  6.66631588e-08], loss = 5.696291811330709e-13\n",
      "Iteration 99: theta = [ 2.03493717e-07 -3.10381643e-07 -2.37430567e-07 -1.59618450e-07\n",
      " -1.84700042e-07], loss = 2.5371188668046325e-13\n",
      "Iteration 100: theta = [ 5.82488503e-08 -1.65136776e-07 -9.21856999e-08 -1.43735832e-08\n",
      " -3.29944909e-07], loss = 1.4823152972424728e-13\n",
      "Iteration 101: theta = [ 1.30052994e-07 -9.33326323e-08 -1.63989844e-07  5.74305604e-08\n",
      " -2.58140766e-07], loss = 1.2245235448671447e-13\n",
      "Iteration 102: theta = [ 9.23926251e-08 -1.30993001e-07 -2.01650212e-07  1.97701916e-08\n",
      " -2.20480397e-07], loss = 1.1536083757757853e-13\n",
      "Iteration 103: theta = [-3.27565837e-08 -5.84379233e-09 -7.65010037e-08  1.44919400e-07\n",
      " -9.53311881e-08], loss = 3.7049215275199844e-14\n",
      "Iteration 104: theta = [ 1.81334709e-10  2.70941261e-08 -4.35630853e-08  1.11981482e-07\n",
      " -1.28269106e-07], loss = 3.1624682934387414e-14\n",
      "Iteration 105: theta = [-4.46112581e-08 -1.76984667e-08 -8.83556781e-08  6.71888892e-08\n",
      " -8.34765137e-08], loss = 2.1592801086360604e-14\n",
      "Iteration 106: theta = [-6.01573603e-08 -2.15236448e-09 -7.28095758e-08  8.27349914e-08\n",
      " -6.79304115e-08], loss = 2.038439461198526e-14\n",
      "Iteration 107: theta = [-6.01573603e-08 -2.15236448e-09 -7.28095758e-08  8.27349914e-08\n",
      " -6.79304115e-08], loss = 2.038439461198526e-14\n",
      "Iteration 108: theta = [-6.32665808e-08 -5.26158493e-09 -7.59187963e-08  7.96257710e-08\n",
      " -6.48211910e-08], loss = 2.0336058353010246e-14\n",
      "Iteration 109: theta = [-6.38884249e-08 -4.63974084e-09 -7.52969522e-08  8.02476151e-08\n",
      " -6.41993469e-08], loss = 2.0334124902641752e-14\n",
      "Iteration 110: theta = [-9.59874709e-08  2.74593052e-08 -4.31979062e-08  4.81485690e-08\n",
      " -3.21003009e-08], loss = 1.518238112250379e-14\n",
      "Iteration 111: theta = [-9.41310726e-08  2.93157035e-08 -4.50543044e-08  5.00049673e-08\n",
      " -3.02439026e-08], loss = 1.516515004961014e-14\n",
      "Iteration 112: theta = [-7.45003653e-08  9.68499619e-09 -6.46850117e-08  3.03742600e-08\n",
      " -4.98746099e-08], loss = 1.3238326710318718e-14\n",
      "Iteration 113: theta = [-6.67002254e-08  1.88485626e-09 -7.24851516e-08  3.81744000e-08\n",
      " -4.20744699e-08], loss = 1.293411579539934e-14\n",
      "Iteration 114: theta = [-6.67002254e-08  1.88485626e-09 -7.24851516e-08  3.81744000e-08\n",
      " -4.20744699e-08], loss = 1.293411579539934e-14\n",
      "Iteration 115: theta = [-6.90141959e-08 -4.29114230e-10 -7.01711812e-08  4.04883705e-08\n",
      " -3.97604995e-08], loss = 1.2907343498195103e-14\n",
      "Iteration 116: theta = [-8.53811898e-08  1.59378797e-08 -5.38041873e-08  2.41213766e-08\n",
      " -2.33935056e-08], loss = 1.1567951054517542e-14\n",
      "Iteration 117: theta = [-6.08772645e-08  4.04418049e-08 -2.93002620e-08  4.86253019e-08\n",
      "  1.11041969e-09], loss = 8.565739284174143e-15\n",
      "Iteration 118: theta = [-7.25973693e-08  2.87217001e-08 -1.75801572e-08  3.69051971e-08\n",
      " -1.06096851e-08], loss = 7.878935002348678e-15\n",
      "Iteration 119: theta = [-6.55653064e-08  3.57537630e-08 -1.05480943e-08  4.39372599e-08\n",
      " -3.57762222e-09], loss = 7.631685460442599e-15\n",
      "Iteration 120: theta = [-3.93391839e-08  9.52764044e-09 -3.67742169e-08  1.77111374e-08\n",
      " -2.98037448e-08], loss = 4.192637936480457e-15\n",
      "Iteration 121: theta = [-3.83131971e-08  1.05536272e-08 -3.78002037e-08  1.87371242e-08\n",
      " -2.87777580e-08], loss = 4.187374692032281e-15\n",
      "Iteration 122: theta = [-4.56028494e-08  1.78432795e-08 -3.05105514e-08  1.14474719e-08\n",
      " -2.14881057e-08], loss = 3.921679538288451e-15\n",
      "Iteration 123: theta = [-4.41449189e-08  1.93012100e-08 -3.19684818e-08  1.29054023e-08\n",
      " -2.00301752e-08], loss = 3.9110517320962085e-15\n",
      "Iteration 124: theta = [-1.84748813e-08 -6.36882767e-09 -6.29844416e-09 -1.27646353e-08\n",
      "  5.63986242e-09], loss = 6.162975658985511e-16\n",
      "Iteration 125: theta = [-1.33690271e-08 -1.14746818e-08 -1.19259002e-09 -7.65878121e-09\n",
      "  1.07457166e-08], loss = 4.859488335539897e-16\n",
      "Iteration 126: theta = [-4.95790379e-09 -3.06355847e-09 -9.60371336e-09  7.52342123e-10\n",
      "  2.33459322e-09], loss = 1.3221385488941203e-16\n",
      "Iteration 127: theta = [-1.74931889e-09  1.45026425e-10 -6.39512846e-09 -2.45624278e-09\n",
      "  5.54317812e-09], loss = 8.073876949132617e-17\n",
      "Iteration 128: theta = [-7.08811206e-10 -8.95481259e-10 -5.35462077e-09 -1.41573509e-09\n",
      "  6.58368580e-09], loss = 7.532548827630619e-17\n",
      "Iteration 129: theta = [-3.05876105e-09 -3.24543110e-09 -3.00467093e-09  9.34214751e-10\n",
      "  4.23373596e-09], loss = 4.771416699281427e-17\n",
      "Iteration 130: theta = [-3.03712500e-09 -3.26706714e-09 -3.02630698e-09  9.55850794e-10\n",
      "  4.21209992e-09], loss = 4.771182640051457e-17\n",
      "Iteration 131: theta = [-5.19775349e-10 -7.49717489e-10 -5.08957321e-10  3.47320045e-09\n",
      "  1.69475026e-09], loss = 1.6026580100911923e-17\n",
      "Iteration 132: theta = [ 5.69617827e-10 -1.83911067e-09  5.80435856e-10  2.38380727e-09\n",
      "  6.05357087e-10], loss = 1.0092692610931597e-17\n",
      "Iteration 133: theta = [-6.26047913e-10 -6.43444926e-10 -6.15229884e-10  1.18814153e-09\n",
      " -5.90308653e-10], loss = 2.944609780805965e-18\n",
      "Iteration 134: theta = [-1.10826333e-09 -1.61229512e-10 -1.33014471e-10  7.05926120e-10\n",
      " -1.08093239e-10], loss = 1.781951240986092e-18\n",
      "Iteration 135: theta = [-1.01182024e-09 -2.57672595e-10 -3.65713883e-11  8.02369202e-10\n",
      " -1.16501565e-11], loss = 1.7354449016686057e-18\n",
      "Iteration 136: theta = [-7.05501118e-10 -5.63991721e-10 -3.42890515e-10  4.96050076e-10\n",
      "  2.94668970e-10], loss = 1.2662878733784963e-18\n",
      "Iteration 137: theta = [-3.62036843e-10 -2.20527447e-10 -6.86354789e-10  1.52585802e-10\n",
      " -4.87953046e-11], loss = 6.764493356839583e-19\n",
      "Iteration 138: theta = [-6.79768030e-11  7.35325932e-11 -3.92294749e-10 -1.41474239e-10\n",
      "  2.45264736e-10], loss = 2.4409280863379873e-19\n",
      "Iteration 139: theta = [-1.95481667e-10  2.01037458e-10 -2.64789884e-10 -1.39693743e-11\n",
      "  1.17759871e-10], loss = 1.6280535519863136e-19\n",
      "Iteration 140: theta = [-1.42789969e-10  1.48345759e-10 -3.17481583e-10  3.87223240e-11\n",
      "  6.50681731e-11], loss = 1.4892328043691273e-19\n",
      "Iteration 141: theta = [-2.12666605e-10  7.84691236e-11 -2.47604947e-10  1.08598960e-10\n",
      " -4.80846266e-12], loss = 1.2450955327299925e-19\n",
      "Iteration 142: theta = [-8.22369863e-11 -5.19604949e-11 -1.17175328e-10 -2.18306588e-11\n",
      "  1.25621156e-10], loss = 3.945012501231206e-20\n",
      "Iteration 143: theta = [-8.22369828e-11 -5.19604984e-11 -1.17175325e-10 -2.18306623e-11\n",
      "  1.25621159e-10], loss = 3.945012501231208e-20\n",
      "Iteration 144: theta = [-5.27205207e-11 -2.24440362e-11 -8.76588628e-11  7.68579991e-12\n",
      "  1.55137621e-10], loss = 3.509401741333344e-20\n",
      "Iteration 145: theta = [ 3.43123967e-12 -7.85957965e-11 -3.15071025e-11 -4.84659604e-11\n",
      "  9.89858612e-11], loss = 1.932892017695589e-20\n",
      "Iteration 146: theta = [ 4.16530924e-11 -4.03739438e-11 -6.97289552e-11 -1.02441076e-11\n",
      "  6.07640084e-11], loss = 1.2024369104312473e-20\n",
      "Iteration 147: theta = [ 6.54470280e-11 -1.65800082e-11 -4.59350197e-11 -3.40380432e-11\n",
      "  3.69700728e-11], loss = 9.193610850948412e-21\n",
      "Iteration 148: theta = [ 4.59002194e-11 -3.61268168e-11 -2.63882111e-11 -5.35848518e-11\n",
      "  1.74232642e-11], loss = 7.283221198316137e-21\n",
      "Iteration 149: theta = [ 2.75401282e-11 -1.77667256e-11 -4.47483023e-11 -3.52247606e-11\n",
      "  3.57833554e-11], loss = 5.597758041824061e-21\n",
      "Iteration 150: theta = [ 2.37307240e-11 -1.39573214e-11 -4.09388981e-11 -3.90341648e-11\n",
      "  3.95927596e-11], loss = 5.525200095371644e-21\n",
      "Iteration 151: theta = [ 2.37307205e-11 -1.39573179e-11 -4.09388946e-11 -3.90341683e-11\n",
      "  3.95927631e-11], loss = 5.5252000953715715e-21\n",
      "Iteration 152: theta = [ 2.93136435e-11 -8.37439493e-12 -4.65218176e-11 -3.34512452e-11\n",
      "  3.40098401e-11], loss = 5.369354735472149e-21\n",
      "Iteration 153: theta = [ 1.25833944e-11  8.35585420e-12 -2.97915685e-11 -1.67209961e-11\n",
      "  5.07400892e-11], loss = 3.9698480332243614e-21\n",
      "Iteration 154: theta = [ 1.08923825e-11  1.00468661e-11 -3.14825804e-11 -1.84120080e-11\n",
      "  4.90490773e-11], loss = 3.9555504091601725e-21\n",
      "Iteration 155: theta = [ 6.87363534e-12  6.02811898e-12 -3.55013276e-11 -2.24307552e-11\n",
      "  4.50303302e-11], loss = 3.874798752316068e-21\n",
      "Iteration 156: theta = [ 3.12581558e-13  1.25891728e-11 -4.20623814e-11 -1.58697014e-11\n",
      "  3.84692764e-11], loss = 3.659561550269615e-21\n",
      "Iteration 157: theta = [ 5.22321943e-12  7.67853489e-12 -4.69730192e-11 -1.09590635e-11\n",
      "  3.35586385e-11], loss = 3.538989745926116e-21\n",
      "Iteration 158: theta = [-1.56552774e-11 -1.31999620e-11 -2.60945224e-11  9.91943333e-12\n",
      "  1.26801417e-11], loss = 1.3594319549370763e-21\n",
      "Iteration 159: theta = [-9.18523764e-12 -6.72992218e-12 -1.96244826e-11  1.63894731e-11\n",
      "  1.91501814e-11], loss = 1.1501250381844784e-21\n",
      "Iteration 160: theta = [-2.81917371e-12 -3.63858249e-13 -2.59905465e-11  1.00234092e-11\n",
      "  1.27841175e-11], loss = 9.474910340271364e-22\n",
      "Iteration 161: theta = [-2.81917024e-12 -3.63854780e-13 -2.59905500e-11  1.00234057e-11\n",
      "  1.27841140e-11], loss = 9.474910340271123e-22\n",
      "Iteration 162: theta = [ 2.31786292e-12 -5.50088794e-12 -2.08535168e-11  4.88637256e-12\n",
      "  1.79211472e-11], loss = 8.155455747076267e-22\n",
      "Iteration 163: theta = [-6.02354798e-12  2.84052296e-12 -1.25121059e-11  1.32277835e-11\n",
      "  9.57973631e-12], loss = 4.676500986636728e-22\n",
      "Iteration 164: theta = [-1.01870075e-12 -2.16432428e-12 -7.50725870e-12  8.22293622e-12\n",
      "  1.45845835e-11], loss = 3.4240774117910354e-22\n",
      "Iteration 165: theta = [ 5.68085683e-12  4.53523330e-12 -8.07701128e-13  1.52337865e-12\n",
      "  7.88502597e-12], loss = 1.1798717345653406e-22\n",
      "Iteration 166: theta = [ 4.34094254e-12  5.87514759e-12 -2.14761542e-12  2.86329294e-12\n",
      "  6.54511167e-12], loss = 1.090103265740924e-22\n",
      "Iteration 167: theta = [1.13183941e-12 2.66604447e-12 1.06148770e-12 6.07239606e-12\n",
      " 3.33600855e-12], loss = 5.751855666283403e-23\n",
      "Iteration 168: theta = [-1.29712387e-12  2.37081188e-13  3.49045098e-12  3.64343278e-12\n",
      "  9.07045272e-13], loss = 2.8019319416945617e-23\n",
      "Iteration 169: theta = [ 5.23072904e-13  2.05727796e-12  1.67025421e-12  1.82323601e-12\n",
      " -9.13151499e-13], loss = 1.1453782195134812e-23\n",
      "Iteration 170: theta = [ 1.04314300e-12  1.53720786e-12  2.19032431e-12  1.30316591e-12\n",
      " -3.93081401e-13], loss = 1.0101430290946608e-23\n",
      "Iteration 171: theta = [ 1.24077658e-12  1.33957428e-12  1.99269073e-12  1.50079948e-12\n",
      " -5.90714977e-13], loss = 9.906145423983874e-24\n",
      "Iteration 172: theta = [ 1.54154987e-12  1.64034758e-12  1.69191744e-12  1.20002619e-12\n",
      " -2.89941682e-13], loss = 9.453829853199517e-24\n",
      "Iteration 173: theta = [ 2.68793668e-13  3.67591374e-13  4.19161234e-13 -7.27300165e-14\n",
      "  9.82814524e-13], loss = 1.354283638094006e-24\n",
      "Iteration 174: theta = [4.15828830e-13 2.20556212e-13 5.66196395e-13 7.43051454e-14\n",
      " 8.35779362e-13], loss = 1.2461854136391773e-24\n",
      "Iteration 175: theta = [5.54089760e-13 3.58817143e-13 4.27935465e-13 2.12566076e-13\n",
      " 6.97518432e-13], loss = 1.1506102657286177e-24\n",
      "Iteration 176: theta = [3.60102573e-13 1.64829955e-13 6.21922652e-13 4.06553263e-13\n",
      " 5.03531244e-13], loss = 9.624598325019103e-25\n",
      "Iteration 177: theta = [4.26039412e-13 9.88931159e-14 5.55985813e-13 3.40616424e-13\n",
      " 5.69468084e-13], loss = 9.407230999536776e-25\n",
      "Iteration 178: theta = [6.14263848e-13 2.87117552e-13 3.67761377e-13 1.52391988e-13\n",
      " 3.81243648e-13], loss = 7.635750310334517e-25\n",
      "Iteration 179: theta = [ 4.06208053e-13  7.90617571e-14  1.59705582e-13 -5.56638069e-14\n",
      "  5.89299443e-13], loss = 5.471339093836539e-25\n",
      "Iteration 180: theta = [ 1.70483419e-13 -1.56662877e-13 -7.60190522e-14 -2.91388441e-13\n",
      "  3.53574808e-13], loss = 2.693091182454482e-25\n",
      "Iteration 181: theta = [ 1.70486888e-13 -1.56659408e-13 -7.60155827e-14 -2.91384972e-13\n",
      "  3.53578278e-13], loss = 2.6930911822137405e-25\n",
      "Iteration 182: theta = [ 1.70479950e-13 -1.56666347e-13 -7.60225216e-14 -2.91391911e-13\n",
      "  3.53571339e-13], loss = 2.6930911838989292e-25\n",
      "Iteration 183: theta = [ 1.40080655e-13 -1.87065641e-13 -4.56232274e-14 -3.21791205e-13\n",
      "  3.23172045e-13], loss = 2.646873727966004e-25\n",
      "Iteration 184: theta = [ 2.12769038e-13 -2.59754024e-13  2.70651557e-14 -2.49102822e-13\n",
      "  2.50483662e-13], loss = 2.382696198036202e-25\n",
      "Iteration 185: theta = [ 1.29323635e-14 -5.99173489e-14 -1.72771519e-13 -4.92661467e-14\n",
      "  5.06469866e-14], loss = 3.859960307088067e-26\n",
      "Iteration 186: theta = [ 5.29003924e-14 -9.98853777e-14 -1.32803490e-13 -9.29811783e-15\n",
      "  1.06789577e-14], loss = 3.0612802406385915e-26\n",
      "Iteration 187: theta = [-3.94302646e-15 -4.30419589e-14 -7.59600716e-14  4.75453010e-14\n",
      "  6.75223766e-14], loss = 1.445791714359112e-26\n",
      "Iteration 188: theta = [ 4.36681941e-14  4.56926164e-15 -2.83488510e-14 -6.59194921e-17\n",
      "  1.99111561e-14], loss = 3.127905160878633e-27\n",
      "Iteration 189: theta = [ 2.43815385e-14 -1.47173940e-14 -9.06219544e-15 -1.93525751e-14\n",
      "  6.24500451e-16], loss = 1.2680966526022128e-27\n",
      "Iteration 190: theta = [ 2.43815385e-14 -1.47173940e-14 -9.06219544e-15 -1.93525751e-14\n",
      "  6.24500451e-16], loss = 1.2680966526022128e-27\n",
      "Iteration 191: theta = [ 1.43756534e-14 -4.71150896e-15 -1.90680804e-14 -9.34669009e-15\n",
      " -9.38138456e-15], loss = 7.6782041247733845e-28\n",
      "Iteration 192: theta = [ 4.88324658e-15 -1.42039158e-14 -9.57567359e-15  1.45716772e-16\n",
      "  1.11022302e-16], loss = 3.1732440584877016e-28\n",
      "Iteration 193: theta = [-8.58688121e-16 -8.46198112e-15 -3.83373888e-15 -5.59621793e-15\n",
      "  5.85295701e-15], loss = 1.5261478440364193e-28\n",
      "Iteration 194: theta = [-5.43488865e-15 -3.88578059e-15  7.42461648e-16 -1.02001740e-15\n",
      "  1.27675648e-15], loss = 4.7859097311920996e-29\n",
      "Iteration 195: theta = [-5.43141920e-15 -3.88925003e-15  7.38992201e-16 -1.02348685e-15\n",
      "  1.28022593e-15], loss = 4.7859193608418215e-29\n",
      "Iteration 196: theta = [-5.73326109e-15 -3.58740815e-15  4.37150316e-16 -7.21644966e-16\n",
      "  9.78384040e-16], loss = 4.7408887113296175e-29\n",
      "Iteration 197: theta = [-4.00894595e-15 -1.86309301e-15  2.16146545e-15  1.00267017e-15\n",
      "  2.70269918e-15], loss = 3.252462642759108e-29\n",
      "Iteration 198: theta = [-3.88751531e-15 -1.98452366e-15  2.04003481e-15  1.12410081e-15\n",
      "  2.82412982e-15], loss = 3.245216331343351e-29\n",
      "Iteration 199: theta = [-1.51788304e-15  3.85108612e-16 -3.29597460e-16 -1.24553146e-15\n",
      "  4.54497551e-16], loss = 4.318828687181e-30\n",
      "Iteration 200: theta = [-1.69829428e-15  2.04697370e-16 -1.49186219e-16 -1.06512021e-15\n",
      "  2.74086309e-16], loss = 4.157965388576056e-30\n",
      "Optimized parameters: [-1.69829428e-15  2.04697370e-16 -1.49186219e-16 -1.06512021e-15\n",
      "  2.74086309e-16]\n",
      "Optimized function value: 4.157965388576056e-30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(theta):\n",
    "    return np.sum(theta**2)\n",
    "\n",
    "def spsa_optimization(f, theta_init, alpha, c, num_iterations):\n",
    "    theta = theta_init\n",
    "    loss_history = []\n",
    "    for k in range(num_iterations):\n",
    "        delta = np.random.choice([-1, 1], size=theta.shape)\n",
    "        # For example, delta = [-1, 1, -1, 1, 1]\n",
    "        loss_plus = f(theta + c * delta)  # real number\n",
    "        loss_minus = f(theta - c * delta)  # real number\n",
    "        # numerator = loss_plus - loss_minus is a real number\n",
    "        # denominator = 2 * c * delta is a vector\n",
    "        g_hat = (loss_plus - loss_minus) / (2 * c * delta)\n",
    "        theta = theta - alpha * g_hat\n",
    "        loss = f(theta)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Iteration {k+1}: theta = {theta}, loss = {loss}\")\n",
    "    return theta, loss_history\n",
    "\n",
    "# Parameter initialization\n",
    "theta_init = np.random.rand(5)\n",
    "alpha = 0.1\n",
    "c = 0.1\n",
    "num_iterations = 200\n",
    "\n",
    "# Perform SPSA optimization\n",
    "theta_opt, loss_history = spsa_optimization(f, theta_init, alpha, c, num_iterations)\n",
    "\n",
    "# Print the optimized results\n",
    "print(\"Optimized parameters:\", theta_opt)\n",
    "print(\"Optimized function value:\", f(theta_opt))\n",
    "\n",
    "# Plot loss value change\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "# plt.yscale('log')\n",
    "plt.title('SPSA Optimization Loss History')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
