{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Parameter Shift Rule by useing automatic differentiation of JAX\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "- Computes the expectation value of the quantum state with respect to the observable $M$ for given parameters $\\theta$.\n",
    "- Strictly follows the description in the overleaf document.\n",
    "\n",
    "---\n",
    "\n",
    "In this paper, we consider the following unconstrained optimization problem,\n",
    "$$\n",
    "\\min _{\\boldsymbol{\\theta} \\in \\mathbb{R}^m} f(\\boldsymbol{\\theta})=\\left\\langle 0\\left|U(\\boldsymbol{\\theta})^{\\dagger} M U(\\boldsymbol{\\theta})\\right| 0\\right\\rangle \\text {. }\n",
    "$$\n",
    "\n",
    "Here, $U(\\boldsymbol{\\theta}) \\in \\mathbb{C}^{2^n \\times 2^n}$ is a parameterized quantum circuit and relies on a collection of classical parameters $\\boldsymbol{\\theta}=\\left(\\theta_1, \\theta_2, \\ldots, \\theta_m\\right) \\in \\mathbb{R}^m$. Without loss generality, it is sufficient to consider the following typical structure:\n",
    "$$\n",
    "U(\\boldsymbol{\\theta})=V_m U_m\\left(\\theta_m\\right) \\cdots V_1 U_1\\left(\\theta_1\\right),\n",
    "$$\n",
    "where $V_j \\in \\mathbb{C}^{2^n \\times 2^n}$ are constant arbitrary quantum circuits, while $U_j\\left(\\theta_j\\right) \\in \\mathbb{C}^{2^n} \\times 2^n$ are rotation-like gates, i.e., $U_j\\left(\\theta_j\\right)=e^{-(i / 2) H_j \\theta_j}$ for some Hermitian generator $H_j \\in \\mathbb{C}^{2^n \\times 2^n}$. It is well know that $U_j\\left(\\theta_j\\right)$ is unitary for any $\\theta_j \\in \\mathbb{R}$ if the generator $H_j$ is Hermitian ${ }^1$. In additional, we assume that $H_j$ is involutory ${ }^2$, i.e., $H_j^2=I$, where $I$ denotes the identity matrix whose size is clear from context. In this case, we have\n",
    "$$\n",
    "U_j\\left(\\theta_j\\right)=e^{-(i / 2) H_j \\theta_j}=\\cos \\left(\\theta_j / 2\\right) I-i \\sin \\left(\\theta_j / 2\\right) H_j\n",
    "$$\n",
    "For proof, see Lemma 2 in appendix. This equation implies that $U_j\\left(\\theta_j\\right)$ is a linear combination of constant matrices $I$ and $H_j$, with the association between $U_j\\left(\\theta_j\\right)$ and $\\theta_j$ being solely through coefficients. Typically, the circuit $U(\\boldsymbol{\\theta})$ is applied to a fixed and easy-toprepare input quantum state $|0\\rangle \\in \\mathbb{C}^{2^n}$ and we obtain the output state $U(\\theta)|0\\rangle \\in \\mathbb{C}^{2^n}$ in a quantum device. Hence, $\\boldsymbol{f}(\\boldsymbol{\\theta})$ is exactly the expectation value of a Hermitian observable $M \\in \\mathbb{C}^{2^n} \\times 2^n$ is measured with respect to that output state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "In this example, we will use JAX's Automatic Differentiation to verify the Parameter Shift Rule is correct.\n",
      "====================================================================================================\n",
      "V_is_unitary_list: [True, True, True, True]\n",
      "H_is_hermitian_list: [True, True, True, True]\n",
      "H_is_unitary_list: [True, True, True, True]\n",
      "Output of exact_cost_naive: 1.2712680101394653\n",
      "Output of exact_cost: 1.2712682485580444\n",
      "The outputs of both functions are equal!\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.linalg\n",
    "from lai_utils import is_unitary, is_hermitian\n",
    "from lai_utils import generate_random_unitary, generate_random_hermitian, generate_random_H_paulis\n",
    "from lai_utils import create_uniform_superposed_state, create_ket_zero_state\n",
    "from lai_utils import U_j\n",
    "\n",
    "key = jax.random.PRNGKey(2)  # Create the initial key for the random number generator\n",
    "\n",
    "print('='*100)\n",
    "print(\"In this example, we will use JAX's Automatic Differentiation to verify the Parameter Shift Rule is correct.\")\n",
    "print('='*100)\n",
    "\n",
    "n = 2  # Number of qubits\n",
    "m = 4  # Number of parameters\n",
    "\n",
    "# |0> state as input\n",
    "input_state = jnp.eye(2**n, dtype=jnp.complex64)[:, 0]  \n",
    "\n",
    "# Generate m different random unitary matrices\n",
    "subkeys = jax.random.split(key, m+1)\n",
    "key = subkeys[m]\n",
    "V_list = [generate_random_unitary(2**n, prng_key=subkeys[i]) for i in range(m)]\n",
    "\n",
    "# Check if each unitary matrix is indeed unitary\n",
    "V_is_unitary_list = [is_unitary(V) for V in V_list]\n",
    "print(f\"V_is_unitary_list: {[value.item() for value in V_is_unitary_list]}\")\n",
    "\n",
    "# Generate m different H matrices\n",
    "subkeys = jax.random.split(key, m+1)\n",
    "key = subkeys[m]\n",
    "H_list = [generate_random_H_paulis(n, prng_key=subkeys[i]) for i in range(m)]\n",
    "\n",
    "# Check if each H matrix is Hermitian\n",
    "H_is_hermitian_list = [is_hermitian(H) for H in H_list]\n",
    "print(f\"H_is_hermitian_list: {[value.item() for value in H_is_hermitian_list]}\")\n",
    "\n",
    "# Check if each H matrix is unitary\n",
    "H_is_unitary_list = [is_unitary(H) for H in H_list]\n",
    "print(f\"H_is_unitary_list: {[value.item() for value in H_is_unitary_list]}\")\n",
    "\n",
    "# Define the observable M matrix\n",
    "key, subkey = jax.random.split(key)\n",
    "M = generate_random_hermitian(2**n, prng_key=subkey) \n",
    "\n",
    "# Define the naive cost function\n",
    "def exact_cost_naive(theta):\n",
    "    U_total = jnp.eye(2**n, dtype=jnp.complex64)\n",
    "    \n",
    "    for i in range(m):\n",
    "        U_theta_i = U_j(theta[i], H_list[i], method='exponential')\n",
    "        U_total = V_list[i] @ U_theta_i @ U_total\n",
    "    \n",
    "    U_total_dagger = U_total.conjugate().T\n",
    "    expectation_value = jnp.vdot(input_state, U_total_dagger @ M @ U_total @ input_state)\n",
    "    \n",
    "    return jnp.real(expectation_value)\n",
    "\n",
    "# Define the practical cost function\n",
    "def exact_cost(theta):\n",
    "    state = input_state\n",
    "\n",
    "    for i in range(m):\n",
    "        U_theta_i = U_j(theta[i], H_list[i], method='exponential')\n",
    "        state = U_theta_i @ state  # Apply U_theta_i to the state vector\n",
    "        state = V_list[i] @ state  # Apply V_list[i] to the state vector\n",
    "    \n",
    "    expectation_value = jnp.vdot(state, M @ state)\n",
    "    \n",
    "    return jnp.real(expectation_value)\n",
    "\n",
    "# Randomly initialize parameters\n",
    "key, subkey = jax.random.split(key)\n",
    "theta = jax.random.normal(subkey, (m,))\n",
    "\n",
    "# Compare the outputs of both functions\n",
    "output_naive = exact_cost_naive(theta)\n",
    "output_practical = exact_cost(theta)\n",
    "\n",
    "print(f\"Output of exact_cost_naive: {output_naive}\")\n",
    "print(f\"Output of exact_cost: {output_practical}\")\n",
    "\n",
    "# Verify if the results are consistent\n",
    "assert jnp.allclose(output_naive, output_practical), \"The outputs are not equal!\"\n",
    "print(\"The outputs of both functions are equal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the definition of the cost function is completed.\n",
    "\n",
    "##  Gradient Calculation\n",
    "\n",
    "- Uses the Parameter Shift Rule to calculate the exact gradient of the cost function.\n",
    "- Verifies the gradient calculated by the Parameter Shift Rule with the gradient computed using JAX's automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly chosen theta: [-0.32004303 -0.63536096  0.5037522   0.39323545]\n",
      "Cost value: 0.4903776943683624\n",
      "JAX - gradient: [ 0.6967335   0.21761885  0.43304354 -0.8293713 ]\n",
      "Parameter shift rule (shift value: 1.5707963267948966) - gradient: [ 0.69673365  0.21761894  0.43304333 -0.8293711 ]\n",
      "Parameter shift rule (shift value: 0.7853981633974483) - gradient: [ 0.69673336  0.21761917  0.43304336 -0.8293713 ]\n",
      "====================================================================================================\n",
      "L2 distance between JAX and PSR gradient (shift: 1.5707963267948966): \n",
      " 3.3979875979639473e-07\n",
      "L2 distance between JAX and PSR gradient (shift: 0.7853981633974483): \n",
      " 3.7961422094667796e-07\n",
      "L2 distance between the two PSR gradients: \n",
      " 4.1429515817981155e-07\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate the exact gradient using the Parameter Shift Rule\n",
    "def psr_grad_function(theta, shift=jnp.pi/2):\n",
    "    # Check if shift is an integer multiple of pi\n",
    "    if jnp.isclose(shift % jnp.pi, 0):\n",
    "        raise ValueError(\"Shift must not be an integer multiple of pi. The best shift value is pi/2\")\n",
    "    \n",
    "    m = len(theta)\n",
    "    gradient = jnp.zeros(m)\n",
    "    \n",
    "    for j in range(m):\n",
    "        theta_shifted = theta.at[j].set(theta[j] + shift)\n",
    "        f_forward = exact_cost(theta_shifted)\n",
    "        \n",
    "        theta_shifted = theta.at[j].set(theta[j] - shift)\n",
    "        f_backward = exact_cost(theta_shifted)\n",
    "        \n",
    "        gradient = gradient.at[j].set((f_forward - f_backward) / (2 * jnp.sin(shift)))\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "# Randomly initialize parameters, 3 common choices\n",
    "theta = jax.random.normal(subkey, (m,))\n",
    "\n",
    "# Create the initial state |+⟩^⊗n\n",
    "# theta = create_uniform_superposed_state(n)\n",
    "\n",
    "# Create the initial state |0⟩^⊗n\n",
    "# theta = create_ket_zero_state(n)\n",
    "\n",
    "print(f\"Randomly chosen theta: {theta}\")\n",
    "\n",
    "# Calculate the cost function value\n",
    "cost_value = exact_cost(theta)\n",
    "print(f\"Cost value: {cost_value}\")\n",
    "\n",
    "# Calculate the gradient of the cost function\n",
    "\n",
    "# 1. Using JAX automatic differentiation\n",
    "jax_grad_function = jax.grad(exact_cost)\n",
    "jax_grad = jax_grad_function(theta)\n",
    "\n",
    "# 2. Using the Parameter Shift Rule\n",
    "SHIFT0 = jnp.pi / 2\n",
    "prs_grad_0 = psr_grad_function(theta, SHIFT0)\n",
    "SHIFT1 = jnp.pi / 4\n",
    "prs_grad_1 = psr_grad_function(theta, SHIFT1)\n",
    "\n",
    "# Print the gradients computed by JAX and the Parameter Shift Rule\n",
    "print(f\"JAX - gradient: {jax_grad}\")\n",
    "print(f\"Parameter shift rule (shift value: {SHIFT0}) - gradient: {prs_grad_0}\")\n",
    "print(f\"Parameter shift rule (shift value: {SHIFT1}) - gradient: {prs_grad_1}\")\n",
    "\n",
    "# Verify if the results are consistent\n",
    "print('='*100)\n",
    "# Calculate the L2 norm distance between the gradients\n",
    "distance1 = jnp.linalg.norm(prs_grad_0 - jax_grad)\n",
    "print(f\"L2 distance between JAX and PSR gradient (shift: {SHIFT0}): \\n {distance1}\")\n",
    "\n",
    "distance2 = jnp.linalg.norm(prs_grad_1 - jax_grad)\n",
    "print(f\"L2 distance between JAX and PSR gradient (shift: {SHIFT1}): \\n {distance2}\")\n",
    "\n",
    "distance3 = jnp.linalg.norm(prs_grad_0 - prs_grad_1)\n",
    "print(f\"L2 distance between the two PSR gradients: \\n {distance3}\")\n",
    "print('='*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RCD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
